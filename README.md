# Neural Networks - Activation Function Analysis

A neural network modelling requires an extensive use of activation functions and the correct activation function can help control and model the output of the problem statement.

This repository has a wide collection of various acivation functions and their python implementations in the script  `activation_functions.py`.
The activation functions covered here are:
- Linear
- Tanh
- Sigmoid
- ReLU
- Leaky ReLU
- PReLU
- Softplus
- Binary Step
- Swish
- Elu
- SiLU
- Mish
- Bent Identity

This list is non exhaustive.